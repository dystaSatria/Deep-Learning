# DENSENET121_Optuna Hyperparameter

---

Bu detaylÄ± dokÃ¼mantasyon, DenseNet121 tabanlÄ± gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma sisteminin tÃ¼m yÃ¶nlerini kapsamaktadÄ±r. Kod'un her bÃ¶lÃ¼mÃ¼, kullanÄ±m senaryolarÄ± ve optimizasyon stratejileri aÃ§Ä±k bir ÅŸekilde aÃ§Ä±klanmÄ±ÅŸtÄ±r.# DenseNet121 ile GeliÅŸmiÅŸ GÃ¶rÃ¼ntÃ¼ SÄ±nÄ±flandÄ±rma - DetaylÄ± Kod Analizi

Bu projede, DenseNet121 Ã¶nceden eÄŸitilmiÅŸ modelini kullanarak kapsamlÄ± bir gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma sistemi geliÅŸtirilmiÅŸtir. Bu dokÃ¼man, kodun her bÃ¶lÃ¼mÃ¼nÃ¼ detaylÄ± olarak aÃ§Ä±klamaktadÄ±r.

## ðŸ“‹ Ä°Ã§erik Tablosu

1. [Proje Genel BakÄ±ÅŸ](#proje-genel-bakÄ±ÅŸ)
2. [KÃ¼tÃ¼phane Ä°mportlarÄ± ve Kurulumlar](#kÃ¼tÃ¼phane-iÌ‡mportlarÄ±-ve-kurulumlar)
3. [Sabit Parametreler](#sabit-parametreler)
4. [Metrik Hesaplama FonksiyonlarÄ±](#metrik-hesaplama-fonksiyonlarÄ±)
5. [DenseNet121 Model FonksiyonlarÄ±](#densenet121-model-fonksiyonlarÄ±)
6. [Makine Ã–ÄŸrenmesi AlgoritmalarÄ±](#makine-Ã¶ÄŸrenmesi-algoritmalarÄ±)
7. [GÃ¶rselleÅŸtirme FonksiyonlarÄ±](#gÃ¶rselleÅŸtirme-fonksiyonlarÄ±)
8. [Optuna Optimizasyon FonksiyonlarÄ±](#optuna-optimizasyon-fonksiyonlarÄ±)
9. [Ana Uygulama Fonksiyonu](#ana-uygulama-fonksiyonu)
10. [KullanÄ±m Ã–rnekleri](#kullanÄ±m-Ã¶rnekleri)

## 1. Proje Genel BakÄ±ÅŸ

### 1.1 Sistem Mimarisi

```
[GÃ¶rÃ¼ntÃ¼ Verisi] â†’ [DenseNet121 Feature Extractor] â†’ [ML AlgoritmalarÄ±] â†’ [Tahmin]
                          â†“
                    [Hiperparametre Optimizasyonu (Optuna)]
                          â†“
                    [Performans Metrikleri]
```

### 1.2 Ana AkÄ±ÅŸ

1. **Veri HazÄ±rlama**: ImageDataGenerator ile veri augmentasyonu
2. **DenseNet121 EÄŸitimi**: Transfer learning ile fine-tuning
3. **Ã–zellik Ã‡Ä±karma**: EÄŸitilen modelden feature extraction
4. **Hibrit Modeller**: ML algoritmalarÄ± ile sÄ±nÄ±flandÄ±rma
5. **Optimizasyon**: Optuna ile hiperparametre ayarlama
6. **DeÄŸerlendirme**: 9 farklÄ± metrik ile analiz

## 2. KÃ¼tÃ¼phane Ä°mportlarÄ± ve Kurulumlar

### 2.1 Temel KÃ¼tÃ¼phaneler

```python
import os                    # Dosya sistemi operasyonlarÄ±
import numpy as np          # Numerical computing
import pandas as pd         # Veri manipÃ¼lasyonu
import matplotlib.pyplot as plt   # GÃ¶rselleÅŸtirme
import seaborn as sns       # GeliÅŸmiÅŸ plotting
```

### 2.2 TensorFlow/Keras Ä°mportlarÄ±

```python
from tensorflow.keras.applications import DenseNet121  # Ã–nceden eÄŸitilmiÅŸ model
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Veri yÃ¼kleme
from tensorflow.keras.models import Model  # Model oluÅŸturma
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout  # Katmanlar
from tensorflow.keras.optimizers import RMSprop, Adam, SGD  # Optimizerlar
from tensorflow.keras.callbacks import EarlyStopping  # Erken durdurma
```

### 2.3 Opsiyonel KÃ¼tÃ¼phane KontrolÃ¼

```python
# LightGBM iÃ§in try-except bloÄŸu
try:
    import lightgbm as lgbm
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    print("Warning: LightGBM is not installed. Skipping LightGBM classifier.")
```

**AÃ§Ä±klama**: Bu yapÄ±, kÃ¼tÃ¼phanenin eksik olmasÄ± durumunda programÄ±n Ã§Ã¶kmesini Ã¶nler ve kullanÄ±cÄ±yÄ± bilgilendirir.

## 3. Sabit Parametreler

```python
IMAGE_SIZE = (150, 150)     # GÃ¶rÃ¼ntÃ¼ boyutu
BATCH_SIZE = 32             # Batch boyutu
EPOCHS = 30                 # EÄŸitim epoch sayÄ±sÄ±
LEARNING_RATE = 0.0001      # Ã–ÄŸrenme oranÄ±
VAL_SPLIT = 0.2             # DoÄŸrulama veri oranÄ±
```

**AÃ§Ä±klama**: Bu parametreler projenin temel konfigÃ¼rasyonunu oluÅŸturur ve kolayca deÄŸiÅŸtirilebilir.

## 4. Metrik Hesaplama FonksiyonlarÄ±

### 4.1 Ã–zel Specificity MetriÄŸi

```python
def calculate_specificity(y_true, y_pred):
    """Calculate specificity for multi-class classification
    
    Specificity = TN / (TN + FP)
    """
    cm = confusion_matrix(y_true, y_pred)
    fp = cm.sum(axis=0) - np.diag(cm)  # False Positives
    tn = cm.sum() - (fp + np.diag(cm) + cm.sum(axis=1) - np.diag(cm))  # True Negatives
    
    specificity = np.zeros_like(tn, dtype=float)
    for i in range(len(specificity)):
        if tn[i] + fp[i] > 0:
            specificity[i] = tn[i] / (tn[i] + fp[i])
        else:
            specificity[i] = 0.0
    
    # AÄŸÄ±rlÄ±klÄ± ortalama dÃ¶ndÃ¼r
    return np.average(specificity, weights=np.bincount(y_true) if len(np.unique(y_true)) > 1 else None)
```

**DetaylÄ± AÃ§Ä±klama**:
- Confusion matrix'ten true negative ve false positive deÄŸerlerini hesaplar
- Her sÄ±nÄ±f iÃ§in specificity hesaplar
- Class imbalance durumunda aÄŸÄ±rlÄ±klÄ± ortalama alÄ±r

### 4.2 Genel Metrik Hesaplama

```python
def calculate_metrics(y_true, y_pred):
    """Calculate all required metrics"""
    metrics = {}
    
    # Temel metrikler
    metrics['accuracy'] = accuracy_score(y_true, y_pred)
    metrics['precision'] = precision_score(y_true, y_pred, average='weighted')
    metrics['recall'] = recall_score(y_true, y_pred, average='weighted')
    
    # F-skorlarÄ± (beta parametreleri ile)
    metrics['f1'] = f1_score(y_true, y_pred, average='weighted')
    metrics['f2'] = fbeta_score(y_true, y_pred, beta=2, average='weighted')
    metrics['f0'] = fbeta_score(y_true, y_pred, beta=0.5, average='weighted')
    
    # GeliÅŸmiÅŸ metrikler
    metrics['specificity'] = calculate_specificity(y_true, y_pred)
    metrics['mcc'] = matthews_corrcoef(y_true, y_pred)
    metrics['kappa'] = cohen_kappa_score(y_true, y_pred)
    
    return metrics
```

**F-Beta Skoru AÃ§Ä±klamasÄ±**:
- **F1 (Î²=1)**: Precision ve Recall'u eÅŸit aÄŸÄ±rlÄ±kta deÄŸerlendirir
- **F2 (Î²=2)**: Recall'a daha fazla Ã¶nem verir
- **F0.5 (Î²=0.5)**: Precision'a daha fazla Ã¶nem verir

### 4.3 SÄ±nÄ±f BazlÄ± Metrik Hesaplama

```python
def calculate_per_class_metrics(y_true, y_pred, class_names):
    """Calculate metrics for each class"""
    n_classes = len(class_names)
    
    # SonuÃ§ dictionary'si baÅŸlat
    results = {
        'class': class_names,
        'accuracy': [], 'precision': [], 'recall': [],
        'specificity': [], 'f0': [], 'f1': [], 'f2': [],
        'kappa': [], 'mcc': []
    }
    
    # Confusion matrix hesapla
    cm = confusion_matrix(y_true, y_pred)
    
    # Her sÄ±nÄ±f iÃ§in metrikleri hesapla
    for i in range(n_classes):
        # Binary classification metrikleri iÃ§in TP, FP, FN, TN hesapla
        tp = cm[i, i]  # True positives
        fp = np.sum(cm[:, i]) - tp  # False positives
        fn = np.sum(cm[i, :]) - tp  # False negatives
        tn = np.sum(cm) - tp - fp - fn  # True negatives
        
        # Metrik hesaplamalarÄ±...
```

**AÃ§Ä±klama**: Her sÄ±nÄ±f iÃ§in ayrÄ± ayrÄ± One-vs-Rest yaklaÅŸÄ±mÄ± ile binary classification metrikleri hesaplar.

## 5. DenseNet121 Model FonksiyonlarÄ±

### 5.1 Feature Extractor OluÅŸturma

```python
def create_feature_extractor():
    # DenseNet121'i ImageNet aÄŸÄ±rlÄ±klarÄ± ile yÃ¼kle
    base_model = DenseNet121(
        weights='imagenet',        # Ã–nceden eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klar
        include_top=False,         # Classification katmanÄ±nÄ± dahil etme
        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)  # RGB gÃ¶rÃ¼ntÃ¼ boyutu
    )
    
    # TÃ¼m katmanlarÄ± dondur (transfer learning)
    for layer in base_model.layers:
        layer.trainable = False
    
    # Global average pooling ekle
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    
    # Feature extractor model oluÅŸtur
    model = Model(inputs=base_model.input, outputs=x)
    
    return model
```

**AÃ§Ä±klama**:
- **include_top=False**: Son classification katmanÄ±nÄ± Ã§Ä±karÄ±r
- **weights='imagenet'**: ImageNet Ã¼zerinde eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klarÄ± kullanÄ±r
- **GlobalAveragePooling2D**: Feature map'leri vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r

### 5.2 Feature Extraction Ä°ÅŸlemi

```python
def extract_features(feature_extractor, data_generator):
    features = []
    labels = []
    
    # TÃ¼m batch'ler iÃ§in feature extraction
    num_batches = len(data_generator)
    for i in range(num_batches):
        x_batch, y_batch = data_generator[i]  # Batch al
        batch_features = feature_extractor.predict(x_batch, verbose=0)  # Feature Ã§Ä±kar
        features.append(batch_features)
        labels.append(np.argmax(y_batch, axis=1))  # One-hot'tan label'a Ã§evir
    
    # TÃ¼m batch'leri birleÅŸtir
    features = np.concatenate(features, axis=0)
    labels = np.concatenate(labels, axis=0)
    
    return features, labels
```

### 5.3 DenseNet121 Model OluÅŸturma

```python
def build_densenet_model(num_classes, dense_neurons=1024, dropout_rate=0.5,
                         optimizer='rmsprop', learning_rate=LEARNING_RATE):
    # Base model yÃ¼kle
    base_model = DenseNet121(
        weights='imagenet',
        include_top=False,
        input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)
    )
    
    # Base model katmanlarÄ±nÄ± dondur
    for layer in base_model.layers:
        layer.trainable = False
    
    # Classification katmanlarÄ± ekle
    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(dense_neurons, activation='relu')(x)  # Ã–zelleÅŸtirilebilir dense layer
    x = Dropout(dropout_rate)(x)                    # Overfitting Ã¶nleme
    predictions = Dense(num_classes, activation='softmax')(x)  # Ã‡Ä±kÄ±ÅŸ katmanÄ±
    
    model = Model(inputs=base_model.input, outputs=predictions)
    
    # Optimizer seÃ§imi
    optimizer_map = {
        'rmsprop': RMSprop(learning_rate=learning_rate),
        'adam': Adam(learning_rate=learning_rate),
        'sgd': SGD(learning_rate=learning_rate)
    }
    opt = optimizer_map.get(optimizer, RMSprop(learning_rate=learning_rate))
    
    # Model compile
    model.compile(
        optimizer=opt,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model
```

**Katman AÃ§Ä±klamalarÄ±**:
- **GlobalAveragePooling2D**: 7x7x1024 â†’ 1024 boyutunda vektÃ¶r
- **Dense(dense_neurons)**: Fully connected layer (Ã¶zelleÅŸtirilebilir)
- **Dropout**: Regularization tekniÄŸi
- **Dense(num_classes, softmax)**: Son sÄ±nÄ±flandÄ±rma katmanÄ±

## 6. Makine Ã–ÄŸrenmesi AlgoritmalarÄ±

### 6.1 SVM Implementation

```python
def train_and_evaluate_svm(train_features, train_labels, test_features, test_labels,
                          kernel='linear', C=1.0, gamma='scale'):
    print(f"Training SVM with {kernel} kernel...")
    
    # SVM model oluÅŸtur
    svm = SVC(
        kernel=kernel,          # Kernel tÃ¼rÃ¼: linear, rbf, poly, sigmoid
        C=C,                   # Regularization parametresi
        gamma=gamma,           # Kernel coefficient
        probability=True,      # Probability prediction iÃ§in
        random_state=42        # Reproducibility iÃ§in
    )
    
    # Model eÄŸit
    svm.fit(train_features, train_labels)
    
    # Tahminler
    y_pred = svm.predict(test_features)
    y_pred_prob = svm.predict_proba(test_features)
    
    # Metrikleri hesapla
    metrics = calculate_metrics(test_labels, y_pred)
    metrics['model'] = f"DenseNet121 + SVM ({kernel})"
    
    return metrics, y_pred, y_pred_prob, svm
```

**SVM Kernel AÃ§Ä±klamalarÄ±**:
- **Linear**: DoÄŸrusal ayrÄ±labilir veriler iÃ§in
- **RBF (Radial Basis Function)**: Non-linear veriler iÃ§in yaygÄ±n
- **Polynomial**: Belirli derecelerde polynomial features
- **Sigmoid**: Neural network benzeri aktivasyon

### 6.2 Gradient Boosting Implementation

```python
def train_and_evaluate_gradient_boosting(train_features, train_labels, test_features, test_labels,
                                         n_estimators=100, learning_rate=0.1, max_depth=3):
    print("Training Gradient Boosting classifier...")
    
    # Gradient Boosting model oluÅŸtur
    gb = GradientBoostingClassifier(
        n_estimators=n_estimators,    # AÄŸaÃ§ sayÄ±sÄ±
        learning_rate=learning_rate,  # Shrinkage parametresi
        max_depth=max_depth,          # AÄŸaÃ§ derinliÄŸi
        random_state=42
    )
    
    # Model eÄŸit ve deÄŸerlendir
    gb.fit(train_features, train_labels)
    y_pred = gb.predict(test_features)
    y_pred_prob = gb.predict_proba(test_features)
    
    metrics = calculate_metrics(test_labels, y_pred)
    metrics['model'] = "DenseNet121 + Gradient Boosting"
    
    return metrics, y_pred, y_pred_prob, gb
```

### 6.3 XGBoost Implementation

```python
def train_and_evaluate_xgboost(train_features, train_labels, test_features, test_labels,
                              n_estimators=100, learning_rate=0.1, max_depth=6):
    print("Training XGBoost classifier...")
    
    # XGBoost parametreleri
    xgb_classifier = xgb.XGBClassifier(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        min_child_weight=1,        # Minimum weight iÃ§in leaf node
        gamma=0,                   # Minimum split loss
        subsample=0.8,             # Row sampling
        colsample_bytree=0.8,      # Column sampling
        objective='multi:softprob', # Multi-class probability
        num_class=len(np.unique(train_labels)),
        random_state=42,
        eval_metric='mlogloss'     # Evaluation metric
    )
    
    # Training ve prediction
    xgb_classifier.fit(train_features, train_labels)
    y_pred = xgb_classifier.predict(test_features)
    y_pred_prob = xgb_classifier.predict_proba(test_features)
    
    metrics = calculate_metrics(test_labels, y_pred)
    metrics['model'] = "DenseNet121 + XGBoost"
    
    return metrics, y_pred, y_pred_prob, xgb_classifier
```

**XGBoost Parametre AÃ§Ä±klamalarÄ±**:
- **subsample**: Her iteration'da kullanÄ±lacak sample oranÄ±
- **colsample_bytree**: Her tree'de kullanÄ±lacak feature oranÄ±
- **gamma**: Leaf node split iÃ§in minimum gain
- **min_child_weight**: Child node iÃ§in minimum weight sum

## 7. GÃ¶rselleÅŸtirme FonksiyonlarÄ±

### 7.1 Training History Plotting

```python
def plot_training_history(history):
    """Plot training and validation accuracy and loss curves"""
    # 2 subplot oluÅŸtur
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))
    
    # Accuracy plot
    ax1.plot(history.history['accuracy'])
    ax1.plot(history.history['val_accuracy'])
    ax1.set_title('DenseNet121 Model Accuracy', fontsize=14)
    ax1.set_ylabel('Accuracy', fontsize=12)
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.legend(['Train', 'Validation'], loc='lower right')
    ax1.grid(True, alpha=0.3)
    
    # Loss plot
    ax2.plot(history.history['loss'])
    ax2.plot(history.history['val_loss'])
    ax2.set_title('DenseNet121 Model Loss', fontsize=14)
    ax2.set_ylabel('Loss', fontsize=12)
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.legend(['Train', 'Validation'], loc='upper right')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('densenet121_training_history.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return fig
```

### 7.2 Confusion Matrix Visualization

```python
def plot_confusion_matrix(y_true, y_pred, class_names, title):
    """Plot confusion matrix with custom styling"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, 
                annot=True,            # HÃ¼crelerde sayÄ±larÄ± gÃ¶ster
                fmt='d',               # Integer format
                cmap='Blues',          # Color map
                xticklabels=class_names, 
                yticklabels=class_names)
    plt.title(title, fontsize=14)
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.tight_layout()
    
    # Dosya kaydet
    filename = title.replace(' ', '_').lower() + '.png'
    plt.savefig(filename, dpi=300, bbox_inches='tight')
    plt.show()
    
    return cm
```

### 7.3 ROC Curve Analysis

```python
def calculate_and_plot_roc(all_models_results, class_names, num_classes):
    plt.figure(figsize=(15, 12))
    
    # ROC AUC deÄŸerlerini sakla
    roc_aucs = {}
    
    # Her model iÃ§in
    for model_name, model_data in all_models_results.items():
        y_true = model_data['y_true']
        y_prob = model_data['y_prob']
        
        # ROC curve ve AUC hesapla
        fpr = dict()
        tpr = dict()
        roc_auc = dict()
        
        # One-hot encoding
        y_true_bin = np.zeros((len(y_true), num_classes))
        for i in range(len(y_true)):
            y_true_bin[i, y_true[i]] = 1
        
        # Her sÄ±nÄ±f iÃ§in ROC
        for i in range(num_classes):
            fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_prob[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])
        
        # Micro-average ROC
        fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_prob.ravel())
        roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
        
        # ROC AUC'u kaydet
        roc_aucs[model_name] = roc_auc["micro"]
        
        # ROC eÄŸrisini Ã§iz
        plt.plot(fpr["micro"], tpr["micro"],
                 label=f'{model_name} (AUC = {roc_auc["micro"]:.4f})',
                 linewidth=2)
    
    # Diagonal Ã§iz (random classifier)
    plt.plot([0, 1], [0, 1], 'k--', linewidth=2)
    
    # Plot ayarlarÄ±
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate', fontsize=12)
    plt.ylabel('True Positive Rate', fontsize=12)
    plt.title('Receiver Operating Characteristic (ROC) Curves for All Models', fontsize=14)
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(True, alpha=0.3)
    plt.savefig('roc_curves_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return roc_aucs
```

## 8. Optuna Optimizasyon FonksiyonlarÄ±

### 8.1 DenseNet Objective Function

```python
def objective_densenet(trial, data_folder):
    """Optuna objective function for optimizing DenseNet121 hyperparameters"""
    
    # Optimizasyon iÃ§in hiperparametreler
    dense_neurons = trial.suggest_categorical('dense_neurons', [512, 1024, 2048])
    dropout_rate = trial.suggest_float('dropout_rate', 0.3, 0.7)
    optimizer_name = trial.suggest_categorical('optimizer', ['rmsprop', 'adam', 'sgd'])
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)
    
    # Data augmentation parametreleri
    aug_rotation = trial.suggest_int('aug_rotation', 5, 20)
    aug_width_shift = trial.suggest_float('aug_width_shift', 0.05, 0.2)
    aug_height_shift = trial.suggest_float('aug_height_shift', 0.05, 0.2)
    aug_zoom = trial.suggest_float('aug_zoom', 0.05, 0.2)
    
    # Veri generatorlarÄ± oluÅŸtur
    train_generator, validation_generator, _ = create_data_generators(
        data_folder,
        batch_size=batch_size,
        aug_rotation=aug_rotation,
        aug_width_shift=aug_width_shift,
        aug_height_shift=aug_height_shift,
        aug_zoom=aug_zoom
    )
    
    # Model oluÅŸtur
    num_classes = len(train_generator.class_indices)
    model = build_densenet_model(
        num_classes=num_classes,
        dense_neurons=dense_neurons,
        dropout_rate=dropout_rate,
        optimizer=optimizer_name,
        learning_rate=learning_rate
    )
    
    # Early stopping
    early_stop = EarlyStopping(
        monitor='val_loss',
        patience=3,
        restore_best_weights=True
    )
    
    # Optuna pruning callback
    pruning_callback = TFKerasPruningCallback(trial, 'val_accuracy')
    
    # Model eÄŸit
    history = model.fit(
        train_generator,
        validation_data=validation_generator,
        epochs=15,  # HÄ±zlÄ± denemeler iÃ§in azaltÄ±lmÄ±ÅŸ
        callbacks=[early_stop, pruning_callback],
        verbose=1
    )
    
    # Validation accuracy'si dÃ¶ndÃ¼r
    return history.history['val_accuracy'][-1]
```

**Optuna AÃ§Ä±klamalarÄ±**:
- **suggest_categorical**: Belirli kategorilerden seÃ§im
- **suggest_float**: SÃ¼rekli deÄŸer aralÄ±ÄŸÄ± (log=True iÃ§in logaritmik)
- **suggest_int**: Integer aralÄ±k
- **TFKerasPruningCallback**: BaÅŸarÄ±sÄ±z denemeleri erken sonlandÄ±rÄ±r

### 8.2 SVM Objective Function

```python
def objective_svm(trial, train_features, train_labels, val_features, val_labels):
    """Optuna objective function for optimizing SVM hyperparameters"""
    
    # Hiperparametre seÃ§imi
    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly', 'sigmoid'])
    C = trial.suggest_float('C', 0.1, 10.0, log=True)
    
    # Linear olmayan kerneller iÃ§in gamma
    if kernel != 'linear':
        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])
    else:
        gamma = 'scale'
    
    # Polynomial kernel iÃ§in ek parametreler
    if kernel == 'poly':
        degree = trial.suggest_int('degree', 2, 5)
        coef0 = trial.suggest_float('coef0', 0.0, 1.0)
        svm = SVC(kernel=kernel, C=C, gamma=gamma, degree=degree, 
                  coef0=coef0, probability=True, random_state=42)
    else:
        svm = SVC(kernel=kernel, C=C, gamma=gamma, probability=True, random_state=42)
    
    # Model eÄŸit ve deÄŸerlendir
    svm.fit(train_features, train_labels)
    val_pred = svm.predict(val_features)
    
    # F1 score dÃ¶ndÃ¼r
    score = f1_score(val_labels, val_pred, average='weighted')
    return score
```

### 8.3 Optimizasyon Ã‡alÄ±ÅŸtÄ±rma

```python
def optimize_densenet(data_folder, n_trials=20):
    """Run Optuna optimization for DenseNet hyperparameters"""
    
    print(f"Starting DenseNet121 hyperparameter optimization with {n_trials} trials...")
    
    # Study oluÅŸtur
    study = optuna.create_study(direction='maximize', study_name='densenet_optimization')
    
    # Optimizasyonu Ã§alÄ±ÅŸtÄ±r
    study.optimize(lambda trial: objective_densenet(trial, data_folder), n_trials=n_trials)
    
    # En iyi deneme
    best_trial = study.best_trial
    
    print(f"Best trial: {best_trial.number}")
    print(f"Best validation accuracy: {best_trial.value:.4f}")
    print("Best hyperparameters:")
    for key, value in best_trial.params.items():
        print(f"    {key}: {value}")
    
    # SonuÃ§larÄ± kaydet
    trials_df = study.trials_dataframe()
    trials_df.to_csv('densenet_optimization_results.csv', index=False)
    
    # Optimizasyon geÃ§miÅŸini gÃ¶rselleÅŸtir
    plt.figure(figsize=(12, 8))
    optuna.visualization.matplotlib.plot_optimization_history(study)
    plt.tight_layout()
    plt.savefig('densenet_optimization_history.png', dpi=300)
    plt.show()
    
    # Parametre Ã¶nemini gÃ¶ster
    plt.figure(figsize=(12, 8))
    optuna.visualization.matplotlib.plot_param_importances(study)
    plt.tight_layout()
    plt.savefig('densenet_param_importances.png', dpi=300)
    plt.show()
    
    return best_trial.params
```

## 9. Ana Uygulama Fonksiyonu

### 9.1 Ana Fonksiyon YapÄ±sÄ±

```python
def run_enhanced_classification_with_optuna(data_folder, run_optimization=True, n_optimization_trials=10):
    print(f"Starting enhanced DenseNet121 classification with Optuna optimization on {data_folder}...")
    
    # 1. BAÅžLANGIÃ‡ KONTROLLERI
    if not os.path.exists(data_folder):
        print(f"Error: Data folder '{data_folder}' not found.")
        return None
    
    start_time = time.time()
    
    # 2. HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU
    if run_optimization:
        best_params = optimize_all_classifiers(data_folder, n_trials=n_optimization_trials)
        # Optimized parametreleri Ã§Ä±kar
        densenet_params = best_params['densenet']
        dense_neurons = densenet_params.get('dense_neurons', 1024)
        dropout_rate = densenet_params.get('dropout_rate', 0.5)
        # ... diÄŸer parametreler
    else:
        # Default parametreler
        dense_neurons = 1024
        dropout_rate = 0.5
# 2. HÄ°PERPARAMETRE OPTÄ°MÄ°ZASYONU
    if run_optimization:
        best_params = optimize_all_classifiers(data_folder, n_trials=n_optimization_trials)
        # Optimized parametreleri Ã§Ä±kar
        densenet_params = best_params['densenet']
        dense_neurons = densenet_params.get('dense_neurons', 1024)
        dropout_rate = densenet_params.get('dropout_rate', 0.5)
        optimizer_name = densenet_params.get('optimizer', 'rmsprop')
        learning_rate = densenet_params.get('learning_rate', LEARNING_RATE)
        
        # Data augmentation parametreleri
        aug_rotation = densenet_params.get('aug_rotation', 10)
        aug_width_shift = densenet_params.get('aug_width_shift', 0.1)
        aug_height_shift = densenet_params.get('aug_height_shift', 0.1)
        aug_zoom = densenet_params.get('aug_zoom', 0.1)
    else:
        # Default parametreler kullan
        dense_neurons = 1024
        dropout_rate = 0.5
        optimizer_name = 'rmsprop'
        learning_rate = LEARNING_RATE
        aug_rotation = 10
        aug_width_shift = 0.1
        aug_height_shift = 0.1
        aug_zoom = 0.1
        best_params = {}
    
    # 3. VERÄ° GENERATORLARÄ° OLUÅžTURMA
    print("Creating data generators with optimized parameters...")
    train_generator, validation_generator, test_generator = create_data_generators(
        data_folder,
        batch_size=BATCH_SIZE,
        aug_rotation=aug_rotation,
        aug_width_shift=aug_width_shift,
        aug_height_shift=aug_height_shift,
        aug_zoom=aug_zoom
    )
    
    # Class bilgilerini al
    class_names = list(train_generator.class_indices.keys())
    num_classes = len(class_names)
    print(f"Number of classes: {num_classes}")
    print(f"Class names: {class_names}")
    
    # 4. SONUÃ‡ SAKLAMAK Ä°Ã‡Ä°N YAPILARI HAZIRLA
    all_models_results = {}
    all_models_metrics = []
    
    # En iyi hibrit model takibi
    best_hybrid_model_name = None
    best_hybrid_model_f1 = -1
    best_hybrid_model_predictions = None
    best_hybrid_model_probabilities = None
    best_hybrid_model_true_labels = None
    
    # 5. DENSENET121 MODEL EÄžÄ°TÄ°MÄ°
    print("\n===== Training Optimized DenseNet121 Model =====")
    model = build_densenet_model(
        num_classes=num_classes,
        dense_neurons=dense_neurons,
        dropout_rate=dropout_rate,
        optimizer=optimizer_name,
        learning_rate=learning_rate
    )
    
    # Model eÄŸitimi
    model, history = fast_train(model, train_generator, validation_generator)
    
    # Training history gÃ¶rselleÅŸtirme
    plot_training_history(history)
    
    # Model deÄŸerlendirme
    densenet_metrics, y_true, y_pred, y_pred_prob = evaluate_densenet(model, test_generator)
    densenet_metrics['model'] = "DenseNet121 (Optimized)"
    
    # SÄ±nÄ±f bazlÄ± metrikler
    densenet_per_class_df = calculate_per_class_metrics(y_true, y_pred, class_names)
    
    # SonuÃ§larÄ± sakla
    all_models_metrics.append(densenet_metrics)
    all_models_results["DenseNet121 (Optimized)"] = {
        "y_true": y_true,
        "y_pred": y_pred,
        "y_prob": y_pred_prob
    }
    
    # Confusion matrix Ã§iz
    plot_confusion_matrix(y_true, y_pred, class_names, "DenseNet121 Confusion Matrix")
    
    # DenseNet F1 skorunu sakla
    densenet_f1 = densenet_metrics['f1']
    
    # 6. Ã–ZELLÄ°K Ã‡IKARMA Ä°ÅžLEMÄ°
    print("\n===== Extracting Features from DenseNet121 =====")
    feature_extractor = create_feature_extractor()
    
    # GeneratorlarÄ± sÄ±fÄ±rla
    train_generator.reset()
    validation_generator.reset()
    test_generator.reset()
    
    # Feature extraction
    print("Extracting training features...")
    train_features, train_labels = extract_features(feature_extractor, train_generator)
    print(f"Training features shape: {train_features.shape}")
    
    print("Extracting validation features...")
    val_features, val_labels = extract_features(feature_extractor, validation_generator)
    print(f"Validation features shape: {val_features.shape}")
    
    print("Extracting test features...")
    test_features, test_labels = extract_features(feature_extractor, test_generator)
    print(f"Test features shape: {test_features.shape}")
    
    # 7. SVM ALGORÄ°TMALARI
    print("\n===== Training and Evaluating Optimized SVM =====")
    
    if run_optimization and 'svm' in best_params:
        # Optimized parametrelerle SVM
        svm_params = best_params['svm']
        kernel = svm_params.get('kernel', 'rbf')
        C = svm_params.get('C', 1.0)
        gamma = svm_params.get('gamma', 'scale')
        
        # Polynomial kernel iÃ§in ek parametreler
        if kernel == 'poly' and 'degree' in svm_params:
            degree = svm_params.get('degree', 3)
            coef0 = svm_params.get('coef0', 0.0)
            # Model eÄŸitimi parametrelerle...
        
        svm_metrics, svm_pred, svm_prob, svm_model = train_and_evaluate_svm(
            train_features, train_labels, test_features, test_labels,
            kernel=kernel, C=C, gamma=gamma
        )
        
        # SonuÃ§larÄ± kaydet
        all_models_metrics.append(svm_metrics)
        all_models_results[f"DenseNet121 + SVM ({kernel})"] = {
            "y_true": test_labels,
            "y_pred": svm_pred,
            "y_prob": svm_prob
        }
        
        # En iyi hibrit modeli kontrol et
        if svm_metrics['f1'] > best_hybrid_model_f1:
            best_hybrid_model_f1 = svm_metrics['f1']
            best_hybrid_model_name = f"DenseNet121 + SVM ({kernel})"
            best_hybrid_model_predictions = svm_pred
            best_hybrid_model_probabilities = svm_prob
            best_hybrid_model_true_labels = test_labels
    else:
        # Optimizasyon yapÄ±lmadÄ±ysa tÃ¼m kernel tÃ¼rlerini dene
        for kernel in ['linear', 'rbf', 'poly', 'sigmoid']:
            svm_metrics, svm_pred, svm_prob, svm_model = train_and_evaluate_svm(
                train_features, train_labels, test_features, test_labels, kernel=kernel
            )
            
            all_models_metrics.append(svm_metrics)
            all_models_results[f"DenseNet121 + SVM ({kernel})"] = {
                "y_true": test_labels,
                "y_pred": svm_pred,
                "y_prob": svm_prob
            }
            
            # En iyi hibrit modeli gÃ¼ncelle
            if svm_metrics['f1'] > best_hybrid_model_f1:
                best_hybrid_model_f1 = svm_metrics['f1']
                best_hybrid_model_name = f"DenseNet121 + SVM ({kernel})"
                best_hybrid_model_predictions = svm_pred
                best_hybrid_model_probabilities = svm_prob
                best_hybrid_model_true_labels = test_labels
    
    # 8. BOOSTING ALGORÄ°TMALARI
    print("\n===== Training and Evaluating Optimized Boosting Methods =====")
    
    # 8.1 Gradient Boosting
    if run_optimization and 'gradient_boosting' in best_params:
        gb_params = best_params['gradient_boosting']
        n_estimators = gb_params.get('n_estimators', 100)
        learning_rate = gb_params.get('learning_rate', 0.1)
        max_depth = gb_params.get('max_depth', 3)
        
        gb_metrics, gb_pred, gb_prob, gb_model = train_and_evaluate_gradient_boosting(
            train_features, train_labels, test_features, test_labels,
            n_estimators=n_estimators,
            learning_rate=learning_rate,
            max_depth=max_depth
        )
    else:
        gb_metrics, gb_pred, gb_prob, gb_model = train_and_evaluate_gradient_boosting(
            train_features, train_labels, test_features, test_labels
        )
    
    # SonuÃ§larÄ± kaydet ve en iyi modeli kontrol et
    all_models_metrics.append(gb_metrics)
    all_models_results["DenseNet121 + Gradient Boosting"] = {
        "y_true": test_labels,
        "y_pred": gb_pred,
        "y_prob": gb_prob
    }
    
    if gb_metrics['f1'] > best_hybrid_model_f1:
        best_hybrid_model_f1 = gb_metrics['f1']
        best_hybrid_model_name = "DenseNet121 + Gradient Boosting"
        best_hybrid_model_predictions = gb_pred
        best_hybrid_model_probabilities = gb_prob
        best_hybrid_model_true_labels = test_labels
    
    # 8.2 XGBoost (EÄŸer mevcutsa)
    if XGBOOST_AVAILABLE:
        if run_optimization and 'xgboost' in best_params:
            xgb_params = best_params['xgboost']
            n_estimators = xgb_params.get('n_estimators', 100)
            learning_rate = xgb_params.get('learning_rate', 0.1)
            max_depth = xgb_params.get('max_depth', 6)
            
            xgb_metrics, xgb_pred, xgb_prob, xgb_model = train_and_evaluate_xgboost(
                train_features, train_labels, test_features, test_labels,
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                max_depth=max_depth
            )
        else:
            xgb_metrics, xgb_pred, xgb_prob, xgb_model = train_and_evaluate_xgboost(
                train_features, train_labels, test_features, test_labels
            )
        
        all_models_metrics.append(xgb_metrics)
        all_models_results["DenseNet121 + XGBoost"] = {
            "y_true": test_labels,
            "y_pred": xgb_pred,
            "y_prob": xgb_prob
        }
        
        if xgb_metrics['f1'] > best_hybrid_model_f1:
            best_hybrid_model_f1 = xgb_metrics['f1']
            best_hybrid_model_name = "DenseNet121 + XGBoost"
            best_hybrid_model_predictions = xgb_pred
            best_hybrid_model_probabilities = xgb_prob
            best_hybrid_model_true_labels = test_labels
    else:
        print("Skipping XGBoost (not installed)")
    
    # 8.3 LightGBM (EÄŸer mevcutsa)
    if LIGHTGBM_AVAILABLE:
        if run_optimization and 'lightgbm' in best_params:
            lgbm_params = best_params['lightgbm']
            n_estimators = lgbm_params.get('n_estimators', 100)
            learning_rate = lgbm_params.get('learning_rate', 0.1)
            num_leaves = lgbm_params.get('num_leaves', 31)
            
            lgbm_metrics, lgbm_pred, lgbm_prob, lgbm_model = train_and_evaluate_lightgbm(
                train_features, train_labels, test_features, test_labels,
                n_estimators=n_estimators,
                learning_rate=learning_rate,
                num_leaves=num_leaves
            )
        else:
            lgbm_metrics, lgbm_pred, lgbm_prob, lgbm_model = train_and_evaluate_lightgbm(
                train_features, train_labels, test_features, test_labels
            )
        
        all_models_metrics.append(lgbm_metrics)
        all_models_results["DenseNet121 + LightGBM"] = {
            "y_true": test_labels,
            "y_pred": lgbm_pred,
            "y_prob": lgbm_prob
        }
        
        if lgbm_metrics['f1'] > best_hybrid_model_f1:
            best_hybrid_model_f1 = lgbm_metrics['f1']
            best_hybrid_model_name = "DenseNet121 + LightGBM"
            best_hybrid_model_predictions = lgbm_pred
            best_hybrid_model_probabilities = lgbm_prob
            best_hybrid_model_true_labels = test_labels
    else:
        print("Skipping LightGBM (not installed)")
    
    # 8.4 CatBoost (EÄŸer mevcutsa)
    if CATBOOST_AVAILABLE:
        catboost_metrics, catboost_pred, catboost_prob, catboost_model = train_and_evaluate_catboost(
            train_features, train_labels, test_features, test_labels
        )
        
        all_models_metrics.append(catboost_metrics)
        
        # Sadece geÃ§erli tahminler varsa ROC analizine ekle
        if catboost_prob is not None:
            all_models_results["DenseNet121 + CatBoost"] = {
                "y_true": test_labels,
                "y_pred": catboost_pred,
                "y_prob": catboost_prob
            }
            
            if catboost_metrics['f1'] > best_hybrid_model_f1:
                best_hybrid_model_f1 = catboost_metrics['f1']
                best_hybrid_model_name = "DenseNet121 + CatBoost"
                best_hybrid_model_predictions = catboost_pred
                best_hybrid_model_probabilities = catboost_prob
                best_hybrid_model_true_labels = test_labels
    else:
        print("Skipping CatBoost (not installed or incompatible)")
    
    # 8.5 AdaBoost
    ada_metrics, ada_pred, ada_prob, ada_model = train_and_evaluate_adaboost(
        train_features, train_labels, test_features, test_labels
    )
    
    all_models_metrics.append(ada_metrics)
    all_models_results["DenseNet121 + AdaBoost"] = {
        "y_true": test_labels,
        "y_pred": ada_pred,
        "y_prob": ada_prob
    }
    
    if ada_metrics['f1'] > best_hybrid_model_f1:
        best_hybrid_model_f1 = ada_metrics['f1']
        best_hybrid_model_name = "DenseNet121 + AdaBoost"
        best_hybrid_model_predictions = ada_pred
        best_hybrid_model_probabilities = ada_prob
        best_hybrid_model_true_labels = test_labels
    
    # 9. TÃœM MODELLERÄ° KARÅžILAÅžTIRMA
    print("\n===== Comparing All Models =====")
    
    if best_hybrid_model_name:
        print(f"Best hybrid model: {best_hybrid_model_name} with F1 score: {best_hybrid_model_f1:.4f}")
        # Baseline ile karÅŸÄ±laÅŸtÄ±r
        if densenet_f1 > best_hybrid_model_f1:
            print(f"Note: DenseNet121 (Optimized) still outperforms hybrid models with F1 score: {densenet_f1:.4f}")
        else:
            print(f"Hybrid model outperforms DenseNet121 (Optimized) (F1: {densenet_f1:.4f})")
    else:
        print("No viable hybrid models found.")
    
    # 10. TÃœM METRÄ°KLERÄ° DATAFRAME'E Ã‡EVÄ°R
    all_metrics_df = pd.DataFrame(all_models_metrics)
    
    # En iyi hibrit model iÃ§in sÄ±nÄ±f bazlÄ± metrikler
    best_hybrid_per_class_df = calculate_per_class_metrics(
        best_hybrid_model_true_labels, best_hybrid_model_predictions, class_names
    )
    
    # Metrik tablolarÄ±nÄ± oluÅŸtur ve kaydet
    create_metrics_tables(densenet_per_class_df, best_hybrid_per_class_df)
    
    # Metrikleri 4 ondalÄ±k basamaÄŸa format la
    for col in ['accuracy', 'precision', 'recall', 'f1', 'f2', 'f0', 'specificity', 'mcc', 'kappa']:
        all_metrics_df[col] = all_metrics_df[col].map(lambda x: f"{x:.4f}")
    
    # Metrikleri gÃ¶ster ve kaydet
    print("\nMetrics for all models:")
    print(all_metrics_df)
    all_metrics_df.to_csv('all_models_metrics.csv', index=False)
    print("Metrics saved to 'all_models_metrics.csv'")
    
    # 11. METRÄ°K GÃ–RSELLEÅžTÄ°RME
    print("\n===== Visualizing Metrics =====")
    
    # String metrikleri float'a Ã§evir
    for col in ['accuracy', 'precision', 'recall', 'f1', 'f2', 'f0', 'specificity', 'mcc', 'kappa']:
        all_metrics_df[col] = all_metrics_df[col].astype(float)
    
    # GÃ¶rselleÅŸtirmeler
    visualize_metrics(all_metrics_df)
    
    # 12. EN Ä°YÄ° HÄ°BRÄ°T MODEL Ä°Ã‡Ä°N CONFUSION MATRIX
    plot_confusion_matrix(
        best_hybrid_model_true_labels,
        best_hybrid_model_predictions,
        class_names,
        f"{best_hybrid_model_name} Confusion Matrix"
    )
    
    # 13. ROC EÄžRÄ°LERÄ°
    print("\n===== Calculating and Plotting ROC Curves =====")
    roc_df, roc_aucs = calculate_and_plot_roc(all_models_results, class_names, num_classes)
    
    # 14. PRECISION-RECALL EÄžRÄ°SÄ°
    print("\n===== Calculating and Plotting Precision-Recall Curve for Best Hybrid Model =====")
    average_precision = plot_precision_recall_curve(
        best_hybrid_model_true_labels,
        best_hybrid_model_probabilities,
        class_names,
        num_classes,
        best_hybrid_model_name
    )
    
    # 15. Ã‡ALIÅžMA SÃœRESÄ° HESAPLAMA
    end_time = time.time()
    execution_time = end_time - start_time
    hours, rem = divmod(execution_time, 3600)
    minutes, seconds = divmod(rem, 60)
    
    print(f"\n===== Enhanced Classification Complete =====")
    print(f"Total execution time: {int(hours)}h {int(minutes)}m {seconds:.2f}s")
    
    # 16. SONUÃ‡LARI RETURN ET
    return {
        'all_metrics_df': all_metrics_df,
        'roc_df': roc_df,
        'densenet_per_class_df': densenet_per_class_df,
        'best_hybrid_per_class_df': best_hybrid_per_class_df,
        'best_hybrid_model_name': best_hybrid_model_name,
        'best_hybrid_model_f1': best_hybrid_model_f1,
        'densenet_f1': densenet_f1,
        'best_params': best_params if run_optimization else None,
        'execution_time': execution_time
    }
```

## 10. KullanÄ±m Ã–rnekleri

### 10.1 Temel KullanÄ±m - Optimizasyonsuz

```python
# HÄ±zlÄ± test iÃ§in optimizayonu kapat
results = run_enhanced_classification_with_optuna(
    "/path/to/dataset",
    run_optimization=False  # Default parametreler kullan
)

print(f"En iyi hibrit model: {results['best_hybrid_model_name']}")
print(f"F1 Score: {results['best_hybrid_model_f1']:.4f}")
```

### 10.2 Optimizasyonlu KullanÄ±m - Az Deneme

```python
# HÄ±zlÄ± optimizasyon
results = run_enhanced_classification_with_optuna(
    "/path/to/dataset",
    run_optimization=True,
    n_optimization_trials=5  # Her algoritma iÃ§in 5 deneme
)
```

### 10.3 Tam Optimizasyon

```python
# En iyi sonuÃ§lar iÃ§in
results = run_enhanced_classification_with_optuna(
    "/path/to/dataset",
    run_optimization=True,
    n_optimization_trials=50  # Her algoritma iÃ§in 50 deneme
)
```

### 10.4 SonuÃ§larÄ± Analiz Etme

```python
# Ã‡alÄ±ÅŸtÄ±rma sonrasÄ± analiz
if results:
    # Genel metrikler
    print("\nGenel Performans:")
    print(results['all_metrics_df'])
    
    # ROC AUC karÅŸÄ±laÅŸtÄ±rmasÄ±
    print("\nROC AUC KarÅŸÄ±laÅŸtÄ±rmasÄ±:")
    print(results['roc_df'])
    
    # DenseNet sÄ±nÄ±f bazlÄ± performans
    print("\nDenseNet SÄ±nÄ±f BazlÄ± Metrikler:")
    print(results['densenet_per_class_df'])
    
    # En iyi hibrit model sÄ±nÄ±f bazlÄ± performans
    print("\nEn Ä°yi Hibrit Model SÄ±nÄ±f BazlÄ± Metrikler:")
    print(results['best_hybrid_per_class_df'])
    
    # Hiperparametre sonuÃ§larÄ± (eÄŸer optimizasyon yapÄ±ldÄ±ysa)
    if results['best_params']:
        print("\nEn Ä°yi Hiperparametreler:")
        for model, params in results['best_params'].items():
            print(f"\n{model}:")
            for param, value in params.items():
                print(f"  {param}: {value}")
```

## 11. Ä°leri Seviye KonfigÃ¼rasyon

### 11.1 Ã–zel Data Augmentation

```python
# Manuel data generator oluÅŸturma
train_gen, val_gen, test_gen = create_data_generators(
    data_folder,
    batch_size=32,
    val_split=0.2,
    aug_rotation=25,        # Daha agresif rotasyon
    aug_width_shift=0.2,    # Daha fazla horizontal shift
    aug_height_shift=0.2,   # Daha fazla vertical shift
    aug_zoom=0.2,           # Daha fazla zoom
    aug_horizontal_flip=True
)
```

### 11.2 Ã–zel Model Parametreleri

```python
# Ã–zel DenseNet modeli
custom_model = build_densenet_model(
    num_classes=10,
    dense_neurons=2048,     # Daha bÃ¼yÃ¼k dense layer
    dropout_rate=0.7,       # Daha yÃ¼ksek dropout
    optimizer='adam',       # Adam optimizer
    learning_rate=0.0005    # FarklÄ± learning rate
)
```

### 11.3 Bellek Optimizasyonu

```python
# GPU bellek bÃ¼yÃ¼mesini etkinleÅŸtir
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
    except RuntimeError as e:
        print(e)

# KÃ¼Ã§Ã¼k batch size kullan
BATCH_SIZE = 16  # VarsayÄ±lan 32 yerine

# DÃ¼ÅŸÃ¼k resolution kullan
IMAGE_SIZE = (128, 128)  # VarsayÄ±lan (150, 150) yerine
```

## 12. Sorun Giderme

### 12.1 YaygÄ±n Hatalar

#### A. Import HatalarÄ±
```python
# KÃ¼tÃ¼phane eksikse bu hata alÄ±nabilir
ImportError: No module named 'lightgbm'

# Ã‡Ã¶zÃ¼m:
pip install lightgbm
```

#### B. GPU Memory HatasÄ±
```python
# TensorFlow GPU memory hatasÄ±
ResourceExhaustedError: OOM when allocating tensor

# Ã‡Ã¶zÃ¼m 1: Batch size azalt
BATCH_SIZE = 16

# Ã‡Ã¶zÃ¼m 2: GPU memory growth etkinleÅŸtir
tf.config.experimental.set_memory_growth(gpu, True)

# Ã‡Ã¶zÃ¼m 3: Mixed precision kullan
policy = tf.keras.mixed_precision.Policy('mixed_float16')
tf.keras.mixed_precision.set_global_policy(policy)
```

#### C. Veri FormatÄ± HatalarÄ±
```python
# YanlÄ±ÅŸ klasÃ¶r yapÄ±sÄ± hatasÄ±
Found 0 images

# Ã‡Ã¶zÃ¼m: KlasÃ¶r yapÄ±sÄ±nÄ± kontrol et
# DoÄŸru yapÄ±:
# dataset/
#   â”œâ”€â”€ train/
#   â”‚   â”œâ”€â”€ class1/
#   â”‚   â”œâ”€â”€ class2/
#   â””â”€â”€ test/
#       â”œâ”€â”€ class1/
#       â””â”€â”€ class2/
```

### 12.2 Performans Optimizasyonu

#### A. HÄ±zlÄ± Test Ä°Ã§in
```python
# Epoch sayÄ±sÄ±nÄ± azalt
EPOCHS = 10

# Optimizasyon deneme sayÄ±sÄ±nÄ± azalt
n_optimization_trials = 3

# Optimizasyonu kapat
run_optimization = False
```

#### B. En Ä°yi SonuÃ§lar Ä°Ã§in
```python
# Epoch sayÄ±sÄ±nÄ± artÄ±r
EPOCHS = 50

# Optimizasyon deneme sayÄ±sÄ±nÄ± artÄ±r
n_optimization_trials = 100

# TÃ¼m opsiyonel kÃ¼tÃ¼phaneleri yÃ¼kle
pip install lightgbm xgboost catboost
```

### 12.3 Debugging Ä°puÃ§larÄ±

#### A. Model EÄŸitimi Ä°zleme
```python
# Verbose=1 ile detaylÄ± Ã§Ä±ktÄ±
history = model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=EPOCHS,
    verbose=1,  # Progress bar gÃ¶ster
    callbacks=[early_stop]
)
```

#### B. Optuna Ä°zleme
```python
# Optuna dashboard kullan
# Terminal'de Ã§alÄ±ÅŸtÄ±r:
# optuna-dashboard sqlite:///study.db

# Veya manual tracking
def objective_with_logging(trial):
    # Log her trial
    print(f"Trial {trial.number} started")
    print(f"Params: {trial.params}")
    
    result = objective_densenet(trial, data_folder)
    
    print(f"Trial {trial.number} result: {result}")
    return result
```

## 13. Proje YapÄ±sÄ± ve Dosya Organizasyonu

### 13.1 Ã–nerilen Proje YapÄ±sÄ±
```
project_name/
â”œâ”€â”€ main_script.py          # Ana kod dosyasÄ±
â”œâ”€â”€ requirements.txt        # Python gereksinimleri
â”œâ”€â”€ README.md              # Bu dokÃ¼mantasyon
â”œâ”€â”€ config.py              # KonfigÃ¼rasyon dosyasÄ± (opsiyonel)
â”œâ”€â”€ data/                  # Veri klasÃ¶rÃ¼
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ test/
â”œâ”€â”€ results/               # SonuÃ§ dosyalarÄ±
â”‚   â”œâ”€â”€ metrics/          # CSV metrik dosyalarÄ±
â”‚   â”œâ”€â”€ plots/            # Graf ve gÃ¶rselleÅŸtirmeler
â”‚   â””â”€â”€ models/           # Kaydedilen modeller
â””â”€â”€ logs/                  # Log dosyalarÄ±
    â””â”€â”€ optimization/      # Optuna log'larÄ±
```

### 13.2 KonfigÃ¼rasyon DosyasÄ± Ã–rneÄŸi
```python
# config.py
class Config:
    # Veri parametreleri
    DATA_FOLDER = "/path/to/dataset"
    IMAGE_SIZE = (150, 150)
    BATCH_SIZE = 32
    VAL_SPLIT = 0.2
    
    # EÄŸitim parametreleri
    EPOCHS = 30
    LEARNING_RATE = 0.0001
    
    # Optimizasyon parametreleri
    RUN_OPTIMIZATION = True
    N_OPTIMIZATION_TRIALS = 20
    
    # Ã‡Ä±ktÄ± klasÃ¶rleri
    RESULTS_DIR = "./results"
    PLOTS_DIR = "./results/plots"
    METRICS_DIR = "./results/metrics"
```

## 14. Gelecek GeliÅŸtirmeler

### 14.1 Potansiyel Ä°yileÅŸtirmeler
1. **Ensemble Methods**: Birden fazla modeli kombine etme
2. **Advanced Augmentation**: Cutout, MixUp, CutMix teknikleri
3. **Attention Mechanisms**: Attention tabanlÄ± feature weighting
4. **Multi-Scale Training**: FarklÄ± resolution'larda eÄŸitim
5. **Semi-Supervised Learning**: Unlabeled data kullanÄ±mÄ±

### 14.2 Model ArÅŸitektura Alternatifleri
```python
# EfficientNet kullanÄ±mÄ±
from tensorflow.keras.applications import EfficientNetB0

# ResNet kullanÄ±mÄ±
from tensorflow.keras.applications import ResNet50V2

# Vision Transformer
# from transformers import ViTFeatureExtractor, ViTForImageClassification
```

## 15. KatkÄ±da Bulunma

### 15.1 Development Setup
```bash
# Development ortamÄ± kurma
git clone <repository>
cd densenet121-classification
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows
pip install -r requirements-dev.txt
```

### 15.2 Code Style
```python
# Black formatter kullan
pip install black
black main_script.py

# Docstring standartlarÄ±
def function_name(param1, param2):
    """
    Brief description.
    
    Args:
        param1 (type): Description
        param2 (type): Description
    
    Returns:
        type: Description
    """
    pass
```

## 16. Lisans ve Referanslar

### 16.1 AÃ§Ä±k Kaynak KÃ¼tÃ¼phaneler
- **TensorFlow/Keras**: Apache License 2.0
- **Scikit-learn**: BSD License
- **Optuna**: MIT License
- **Pandas**: BSD License
- **NumPy**: BSD License

### 16.2 Bilimsel Referanslar
1. DenseNet: Huang, G., et al. "Densely connected convolutional networks." (2017)
2. Optuna: Akiba, T., et al. "Optuna: A next-generation hyperparameter optimization framework." (2019)
3. Transfer Learning: Pan, S. J., & Yang, Q. "A survey on transfer learning." (2010)
